
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta author="Joey Carlisle">
    <title>CS 581: Project 4</title>
    <link rel="icon" href="img/eye.png">
    <link rel="stylesheet" type="text/css" href="../styles.css" />
</head>
<body>
    <header>
        <a class="inline" href="../index.html">&lt- Back Home</a>
        <br>
        <nav>
            <h1>CS 581: Project 4 - Diffusion Models</h1>
        </nav>
        <h2 class="tab">Objective:</h2>
        <h3 class="tab">
            The goal of this project is to help you build a practical understanding of diffusion models, from the forward noising process to iterative denoising and sampling. 
        </h3>
        <ul>
            <li class="tab">In Part 0, Setup.</li>
            <li class="tab">In Part 1, you will implement the forward diffusion process and experiment with both traditional denoising (Gaussian filtering) and a pre-trained diffusion model to understand how noise is added and removed. </li>
            <li class="tab">In Part 2, you will work with a pre-trained diffusion model to perform sampling, explore classifier-free guidance, and apply the model to a creative task. </li>
            <li class="tab">In Part 3, (graduate students only), you will explore a recent research paper on visual anagrams (CVPR 2024) and generate multi-view optical illusions using diffusion models. </li>
            <li class="tab">In Part 4, (optional, extra credit 33%), you will train your own denoising U-Net on MNIST and implement a full DDPM pipeline. </li>
        </ul>
    </header>
    <main>
        <div class="tab">
            <h2>Part 0: Setup </h2>
            <h3>The DeepFloyd IF diffusion model is going to be used in this project. DeepFloyd is trained and released by Stability AI. It's first stage generates images with size 64 x 64, the second stage take the outputs of the first stage and produces images with size 256 x 256.</h3>
            <h3>Tasks: Text-to-image sampling with provided prompts</h3>
            <ul>
                <li>For each of the three provided text prompts, generate an image using the DeepFloyd IF model and display both the prompt and the generated result. Briefly comment on the quality of each output and how well it matches the prompt. 
                    Try at least two different values of num_inference_steps and mention anything you observe from changing this parameter.</li>
                <li>Report the random seed you use. You should use the same seed for all parts of the project that involve image generation. </li>
            </ul>
            <div class="images">
                <article class="center">
                    <div >
                        <img src="img/num_inf_20.png" alt="3 ai generated images" width="600" height="125"/>
                    </div>
                    <div>
                        <p>For all the generated images I used 150 as the random seed. The image above was generated using the first stage of the model with a number of inference steps of 20. Due to the size of the images, it is hard to see some of the detail, but you can see the images match the prompts. 
                            The snowy mountain image has one orange point on the mountain that stands out as being off but could possibly be sunlight. The man wearing a hat image looks really good without noticeable mistakes. The rocket ship has a tail behind the flames
                                and the left wing looks solid red where the right is white with a red outline.
                                The 3 following images are the same as above only separated to show more detail. </p>
                    </div>
                    <div >
                        <img src="img/stage1_inf_20_oil.png" alt="oil painting" width="580" height="200"/>
                    </div>
                    <div>
                        <img src="img/stage1_inf_20_hat.png" alt="man wearing a hat" width="300" height="200"/>
                    </div>
                    <div>
                        <img src="img/stage1_inf_20_rocket.png" alt="rocket ship" width="200" height="200"/>
                    </div>
                    <div>
                        <p>The images below were generated using the second stage of the model with a number of inference steps of 20.</p>
                    </div>
                    <div>
                        <img src="img/num_infer_20.png" alt="3 ai generated images" width="600" height="225"/>
                    </div>
                    <div>
                        <p>With the larger size of the images, it is easier to see the details and you can clearly see the images match the prompts. The snowy mountain image has more orange on the mountains as sunlight reflection. 
                            The man wearing a hat image is by far the best in my opinion. The rocket ship has the same issues as the first stage image did. </p>
                    </div>
                    <div>
                        <p>The images below were also generated using the second stage of the model but with a number of inference steps of 100.</p>
                    </div>
                    <div>
                        <img src="img/num_infer_100.png" alt="3 ai generated images" width="600" height="225"/>
                    </div>
                    <div>
                        <p>I was surprised to see the images changed with a higher number of inference steps. I thought it would simply cause the image to be refined more, but I didn't realize they would change completely.  
                            The snowy mountain image feels off with the layer look on the side of the mountain and what looks like the side of a building on the lower left but is a hill behind. The man wearing a hat image again 
                            looks the best of the three to me. The rocket ship has two lines pointing up from the wings making the image seem as if the ship is landing like space-X ships rather than taking off, but the prompt 
                            doesn't say any action only a rocket ship. Both the first and second stages have the ship flying within one would assume earth's atmosphere instead of outer space.</p>
                    </div>
                    <div>
                        <p>The images below were again generated using the second stage of the model with a number of inference steps of 500.</p>
                    </div>
                    <div>
                        <img src="img/num_infer_500.png" alt="3 ai generated images" width="600" height="225"/>
                    </div>
                    <div>
                        <p>To me, I find the last three at inference steps of 500 to the worst. The snowy mountain image seems more cartoonish to me. The man wearing a hat image again has great details around the eyes, but the 
                            teeth area is off making it look like he is wearing a mouthpiece/guard. The rocket ship looks very bad to me. It is very cartoonish with multiple issues with the wings, fuselage, and exhaust. </p>
                    </div>
                </article>
            </div>
            
        </div>
        <br>
        <div class="tab">
            <h2>Part 1: Understanding the Forward & Reverse Processes</h2>
            <h3>1.1 Forward Process (Adding Noise)</h3>
            <h4>The forward process gradually adds Gaussian noise to a clean image according to a predefined noise schedule. Your goal is to implement this process and visualize how the image degrades as noise increases.</h4>
            <h3>Tasks:</h3>
            <ul>
                <li>Implement a function noisy_im = forward(im, t) that takes an image and a timestep and returns the noisy version of the image.</li>
                <li>You need to use the list alphas_cumprod variable.</li>
                <li>Apply your function to a test image at t = 250, 500, 750</li>
            </ul>
            <h3>1.2 Traditional Denoising (Gaussian Blur)</h3>
            <h4>Before using a diffusion model to denoise, we will first try a traditional method: Gaussian filtering, which has been implemented in Project 1 - Part 2(d).<br>You will see that while Gaussian filtering 
                can smooth out some noise, it also destroys important image details, especially when the noise level is high.</h4>
            <h3>Tasks:</h3>
            <ul>
                <li>Take the three noisy images from Part 1.1 at timesteps $t = 250, 500, 750$</li>
                <li>For each noisy image, apply Gaussian filtering. Tune the blur parameters (kernel size or sigma) to get your best denoised result for each timestep. </li>
                <li>For t = 250, 500, 750, a figure (or three figures) showing: <br>Top row: noisy images at each timestep. <br>Bottom row: your best Gaussian-denoised images for the same timesteps.</li>
            </ul>
            <div class="images">
                <article class="center">
                    <br>
                    <div>
                        <img src="img/blur_k5_s1.png" alt="Lighthouse" width="600" height="350"/>
                    </div>
                    <div>
                        <p>For the Gaussian filtering I used a kernel size of 5 and a sigma of 1. I found these to be the best denoised results for each timestamp.</p>
                    </div>
                    <br>
                </article>
            </div>
            <h3>1.3 One-Step Denoising (Using a Pretrained UNet)</h3>
            <h4>In this part, you will perform one-step denoising using a pretrained diffusion model (denoising UNet, stage_1.unet) to do denoising. The UNet has been trained on a very large dataset of (x<sub>0</sub>, x<sub>t</sub>)
                 pairs of images, thus, it can be used to recover Gaussian noise from an image. </h4>
            <h3>Tasks:</h3>
            <ul>
                <li>Take the three noisy images from Part 1.1 at timesteps t = 250, 500, 750</li>
                <ul>
                    <li>Use the pretrained UNet, predict the noise and then denoise the imaage using Equation 1.3. </li>
                    <li>Show the original image, the noisy image, and the estimate of the original (clean) image</li>
                </ul>
            </ul>
            <div class="images">
                <article class="center">
                    <br>
                    <div>
                        <img src="img/One_step.png" alt="Lighthouse" width="600" height="600"/>
                    </div>
                </article>
            </div>
            <h3>1.4 Iterative Denoising</h3>
            <h4>In Part 1.3 you saw that a single denoising step can already recover a reasonable image when the noise level is moderate. As we discussed in class, diffusion models are designed to denoise gradually, 
                starting from very noisy images and stepping toward a clean sample. <br>Ideally, we should start with noise $x_{1000}$ at time step T = 1000, denoise for one step to get an estimate of x<sub>999</sub>, 
                and keep doing this untile we get x<sub>0</sub>. This means that we need to run the diffusion model (UNet) 1000 times, which is very slow! <br>In practice, we can speed up by skipping steps, you may get 
                this idea in Part 1.3 where you may find that using smaller T, the denoising result is not bad. <br>To skip steps, we can use a list of timesteps strided_timesteps, which is much shorter than the full list 
                of 1000 timesteps. strided_timesteps[0] will correspond to the noisiest image (and thus the largest t) and strided_timesteps[-1] will correspond to a clean image (and thus t = 0). One option of constructing 
                this list is by introducing a regular stride step (e.g. stride of 30).</h4>
            <h3>Tasks:</h3>
            <p>Using i_start = 10</p>
            <ul>
                <li>Create strided_timesteps: a list of monotonically decreasing timesteps, starting at 990, with a stride of 30, eventually reaching 0. Also initialize the timesteps using the function stage_1.scheduler.set_timesteps(timesteps=strided_timesteps)</li>
                <li>Complete the iterative_denoise function </li>
                <li>Show the noisy image every 5th loop of denoising (it should gradually become less noisy)</li>
                <li>Show the final predicted clean image, using iterative denoising</li>
                <li>Show the predicted clean image using only a single denoising step, as was done in the previous part. This should look much worse.</li>
                <li>Show the predicted clean image using gaussian blurring, as was done in Part 1.2.</li>
            </ul>
            <div class="images">
                <article class="center">
                    <br>
                    <div>
                        <img src="img/noisyImages.png" alt="Lighthouse" width="600" height="300"/>
                    </div>
                    <div>
                        <img src="img/1_4Images.png" alt="Lighthouse" width="600" height="200"/>
                    </div>
                </article>
            </div>
        </div>
        <br>
        <div class="tab">
            <h2>Part 2: Understanding the Forward & Reverse Processes</h2>
            <h3>2.1 Diffusion Model Sampling</h3>
            <h4>In Part 1.4, we use the diffusion model to denoise an image. We can also use iterative_denoise function to generate images from random noise. We can do this by setting i_start = 0 and passing in random noise. This effectively denoises pure noise. </h4>
            <h3>Tasks:</h3>
            <ul>
                <li>Generate images from random noise, and show 5 results of "a high quality photo". Show 5 sampled images.</li>
            </ul>
            <div class="images">
                <article>
                    <div class="center">
                        <img src="img/1_5.png" alt="5 generated images" width="600" height="350"/>
                    </div>
                    <div class="center">
                        <p>Top-left - it looks like lily pads in a pond maybe<br>
                        Top- middle - a childs doll <br>
                        Top-right - a childs valcano assignment<br>
                        Bottom-left - a person in the woods on a winter day<br>
                        Bottom-middle - a skunk curled up maybe</p>
                    </div>
                </article>
            </div>
            <h3>2.2 Classifier Free Guidance</h3>
            <h4>You may have observed that some of the generated images in the last section (Part 2.1) are not very good. In order to improve image quality (may at the cost of image diversity), we can use a technique called Classifier-Free Guidance (CFG). </h4>
            <h3>Tasks:</h3>
            <ul>
                <li>Implement the iterative_denoise_cfg function, identical to the iterative_denoise function but using classifier-free guidance (CFG). To get an unconditional noise estimate, we can just pass an empty prompt embedding to the diffusion model (the model 
                    was trained to predict an unconditional noise estimate when given an empty text prompt).</li>
                <li>Show 5 images of "a high quality photo" with a CFG scale of &gamma; = 7</li>
            </ul>
            <div class="images">
                <article>
                    <div class="center">
                        <img src="img/1_6.png" alt="5 generated images" width="600" height="350"/>
                    </div>
                    <div class="center">
                        <p>These came out a lot better than other ones.<br>
                        Top-left - a girl<br>
                        Top- middle - a pier and sunset<br>
                        Top-right - a guy<br>
                        Bottom-left - a laje and mountains during sunset<br>
                        Bottom-middle - a girl</p>
                    </div>
                </article>
            </div>
            <h3>2.3 Image-to-image Translation</h3>
            <h4>Here, we're going to take the original test image, noise it a little, and force it back onto the image manifold without any conditioning. Effectively, we're going to get an image that is similar to the test image (with a low-enough noise level). This follows the SDEdit algorithm.</h4>
            <h3>Tasks:</h3>
            <ul>
                <li>Edits of the test image, using the given prompt at noise levels [1, 3, 5, 7, 10, 20] with text prompt "a high quality photo"</li>
                <li>Edits of 2 of your own test images, using the same procedure.</li>
            </ul>
            <div class="images">
                <article>
                    <div class="center">
                        <figure class="center">
                            <img src="img/campanile.jpg" alt="Lighthouse" width="200" height="250"/>
                            <img src="img/1_7Lighthouse.png" alt="Lighthouse" width="600" height="300"/>
                            <figcaption>The original lighthouse image</figcaption>
                        </figure>
                        <figure class="center">
                            <img src="img/dog.jpg" alt="A dog and a guy" width="200" height="250">
                            <img src="img/1_7DogAndMe.png" alt="Dog and me" width="600" height="300"/>
                            <figcaption>My late dog and me</figcaption>
                        </figure>
                        <figure class="center">
                            <img src="img/resort.jpg" alt="High Hampton resort" width="275" height="200">
                            <img src="img/1_7Resort.png" alt="Resort" width="600" height="300"/>
                            <figcaption>A resort I worked at in North Carolina</figcaption>
                        </figure>
                    </div>
                </article>
            </div>
        </div>
        <br>
        <div class="tab">
            <h2>Part 3: Visual Anagrams (Graduate Students)</h2>
            <h3>In this part, we will implement Visual Anagrams (CVPR 2024) and create optical illusions with diffusion models. For example, the task is to create an image that looks like "an oil painting of an old man", but when flipped upside down will reveal "an oil painting of people around a campfire".</h3>
            <h3>Tasks:</h3>
            <ul>
                <li>Implemented visual_anagrams function</li>
                <li>A visual anagram where on one orientation "an oil painting of people around a campfire" is displayed and, when flipped, "an oil painting of an old man" is displayed. </li>
                <li>2 more illusions of your choice that change appearance when you flip it upside down (see the appendix to generate your own prompts) </li>
            </ul>
            <div class="images">
                <article>
                    <div class="center">
                        <img src="img/part3OldMan.png" alt="An old man, flipped to a campfire" width="600" height="300"/>
                    </div>
                    <div class="center">
                        <img src="img/part3Barista.png" alt="Barista flipped to a dog" width="600" height="300"/>
                    </div>
                    <div class="center">
                        <img src="img/part3Skull.png" alt="waterfall flipped to a skull" width="600" height="300"/>
                    </div>
                </article>
            </div>  
        </div>
    </main>
</body>
</html>
